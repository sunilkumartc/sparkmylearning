
import org.apache.spark.sql.sources.v2._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import org.apache.spark.sql.sources.v2.reader._
import scala.collection.JavaConverters._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.sources._

class DefaultSource extends DataSourceV2 with ReadSupport {

  def createReader(options: DataSourceOptions) = {
    new SimpleMysqlDataSourceReader()
  }
}

class SimpleMysqlDataSourceReader() extends DataSourceReader with SupportsPushDownFilters{

  var pushedFilters:Array[Filter] = Array[Filter]()
  def readSchema() = {
    val columnNames = Array("user")
    val structFields = columnNames.map(value â‡’ StructField(value, StringType))
    StructType(structFields)
  }

  def pushFilters(filters:Array[Filter]) = { 
     println(filters.toList)
     pushedFilters = filters
     pushedFilters
  }

  def createDataReaderFactories = {
    val sparkContext = SparkSession.builder.getOrCreate().sparkContext

    val factoryList = new java.util.ArrayList[DataReaderFactory[Row]]
    factoryList.add(new SimpleMysqlDataSourceReaderFactory(pushedFilters))
    factoryList
  }

}

class SimpleMysqlDataSourceReaderFactory(pushedFilters:Array[Filter]) extends DataReaderFactory[Row] {

  def createDataReader = new SimpleMysqlDataReader(pushedFilters:Array[Filter])
}

class SimpleMysqlDataReader(pushedFilters:Array[Filter]) extends DataReader[Row] {

  var iterator: Iterator[Row] = null

  val getQuery:String = {
   if(pushedFilters == null || pushedFilters.isEmpty)
     "(select user from user)a"
   else {
    pushedFilters(1) match {
    case filter : EqualTo =>
    val condition = s"${filter.attribute} = '${filter.value}'"
    s"(select user from user where $condition)a"
    case _ =>"(select user from user)a"
    }
   }
  }

  def next =  {
   if(iterator == null) {
     val url = "jdbc:mysql://localhost/mysql"
     val user = "root"
     val password = "abc123"

     val properties = new java.util.Properties()
     properties.setProperty("user",user)
     properties.setProperty("password",password)


    val sparkSession = SparkSession.builder.getOrCreate()
    val df = sparkSession.read.jdbc(url,getQuery,properties)
    val rdd = df.rdd
    val partition = rdd.partitions(0)
    iterator = rdd.iterator(partition, org.apache.spark.TaskContext.get())
   }
   iterator.hasNext
  }

  def get = {
     iterator.next()
  }
  def close() = Unit
}

